import urllib
import urllib2
import string
import HTMLParser
from bs4 import BeautifulSoup
import sys
import re
import threading
import time

class FetchUrl(threading.Thread):
	def __init__(self, url, output, lock):
		threading.Thread.__init__(self)
		self.url = url
		self.output = output
		self.lock = lock

	def run(self):
		if self.url:
			htmlSrc = urllib2.urlopen(urllib2.Request(self.url)).read()
			parser = BeautifulSoup(htmlSrc)
			vList = parser.findAll('a')
			for vName in vList:
				self.lock.acquire(1)
				self.output[vName.text] = vName.text
				self.lock.release()


def fetchCVE(url, output):
	if url[-1] != '/':
		url += '/'
	lock = threading.Lock()
	htmlSrc = urllib2.urlopen(urllib2.Request(url)).read()
	parser = BeautifulSoup(htmlSrc)
	cList = parser.findAll(attrs = {'valign' : 'top'})
	threadList = []
	for cName in cList:
		suffix = cName.findNext('a')['href']
		if '.html' in suffix:
			t1 = FetchUrl(url + suffix, output, lock)
			t1.start()
			threadList.append(t1)
	while len(threadList):
		t2 = threadList.pop()
		t2.join()

def fetchRapid7(url, output):
	pass

def fetchCVEDetails(url, output):
	if url[-1] != '/':
		url += '/'
	# productIDList = [17448, 20205, 22000, 19997, 27152, 21060, 28426, 24666, 16896, 13517, 27638, 24656]
	productIDList = [19997]
	lock = threading.Lock()
	threadList = []
	for pID in productIDList:
		aurl = url + str(pID) + '/'
		htmlSrc = urllib2.urlopen(urllib2.Request(aurl)).read()
		parser = BeautifulSoup(htmlSrc).find('table', 'stats')
		total = parser.find('th', text = 'Total')
		preLinks = total.findNext('td', 'num')
		print total.text.strip(), ':', preLinks.text.strip()
		links = preLinks.find_next_siblings('td', 'num', recursive = False)
		for link in links:
			if link.text.strip().isdigit():
				print link.findNext('a')['href']
				t1 = FetchUrl('http://www.cvedetails.com/' + link.findNext('a')['href'], output, lock)
				t1.start()
				threadList.append(t1)
		while len(threadList):
			t2 = threadList.pop()
			t2.join()


def test(url):
	html_src = urllib2.urlopen(urllib2.Request(url)).read()
	parser = BeautifulSoup(html_src)
	items = parser.findAll('a')
	for item in items:
		print item.text, item.findNext()

def main():
	vdict = {}
	t0 = time.time()
	# fetchCVE("https://cassandra.cerias.purdue.edu/CVE_changes/", vdict)
	# fetchRapid7("http://www.rapid7.com/db/search?utf8=%E2%9C%93&q=android&t=v", vdict)
	fetchCVEDetails("http://www.cvedetails.com/product/", vdict)
	print "it took", time.time() - t0
	print len(vdict)
	# test('https://cassandra.cerias.purdue.edu/CVE_changes/CVE.2001.11.html')

if __name__ == '__main__':
	main()